import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score
import numpy as np
import pandas as pd


# Creating a function for percentage labeled bar plot
def labeled_barplots(data, feature, perc=False, n=None, title=None):
    total = len(data[feature])
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(data=data, x=feature, palette='Paired',
                       order=data[feature].value_counts().index[:n].
                       sort_values(),)
    for p in ax.patches:
        if perc == True:
            label = '{:.1f}%'.format(100 * p.get_height() / total)
        else:
            label = p.get_height()
        x = p.get_x() + p.get_width() / 2
        y = p.get_height()

        ax.annotate(label, (x, y), ha='center', va='center', size=12,
                    xytext=(0, 5), textcoords='offset points',)
    if title:
        plt.title(title)
    plt.show()
    
    
# function to compute different metrics to check performance of a regression model
def model_performance_classification(model, predictors, target, threshold = 0.5):
    """
    Function to compute different metrics to check regression model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred_proba = model.predict_proba(predictors)[:, 1]
    # convert the probability to class
    pred_class = np.round(pred_proba > threshold)
    acc = accuracy_score(target, pred_class)  # to compute acuracy
    recall = recall_score(target, pred_class)  # to compute recall
    precision = precision_score(target, pred_class)  # to compute precision
    f1 = f1_score(target, pred_class)  # to compute F1 score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1-score": f1
        },
        index=[0])
    conf = confusion_matrix(target, pred_class)
    plt.figure(figsize=(5, 5))
    sns.heatmap(conf, annot=True, fmt="g")
    plt.xlabel("Predicted label")
    plt.ylabel("Actual label")
    plt.show()
 
    return df_perf

# Function to draw the feature importance diagram
def draw_importance(model, predictors):
    feature_names = predictors.columns.to_list() #get the feature names
    importances = model.feature_importances_  # get the feature importance
    indices = np.argsort(importances)   # sort the feature importance

    plt.figure(figsize = (10, 10))
    plt.title("Feature Importances")
    plt.barh(range(len(indices)), importances[indices], color = "violet", 
             align = "center")
    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
    plt.xlabel("Relative Importance")
    plt.show()
    
    
    
    
# Creating a function for bar plots
def histogram_boxplot(data, feature, figsize = (12, 7), kde = False, bins = None):
    f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, sharex = True, gridspec_kw 
                                           = {'height_ratios': (0.25, 0.75)},figsize = figsize)
    
# creating subplots
    sns.boxplot(data = data, x = feature, ax = ax_box2, showmeans = True, color = 'violet')
# The above creates a boxplot, with a star indicating the mean value
    sns.histplot(data = data, x = feature, kde = kde, ax = ax_hist2, bins = bins, palette = 'winter') if bins else sns.histplot(data = data, x = feature, kde = kde, ax = ax_hist2)
    
# Adding mean to the histogram
    ax_hist2.axvline(data[feature].mean(), color = 'green', linestyle = '--')
    
# Adding median to the histogram
    ax_hist2.axvline(data[feature].median(), color = 'black', linestyle = '-')